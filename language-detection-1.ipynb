{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T19:23:07.550839Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset merging\n",
    "\n",
    "df = pd.DataFrame(columns=['Text','Language'])\n",
    "for filename in os.listdir('/kaggle/input/lang-dataset'):\n",
    "    f = os.path.join('/kaggle/input/lang-dataset', filename)\n",
    "    if 'csv' in filename:\n",
    "        if 'sentences' not in filename:\n",
    "            df1 = pd.read_csv(f)\n",
    "            if 'language' in df1.columns:\n",
    "                df1=df1.rename(columns={'language':'Language'})\n",
    "            df = pd.concat([df,df1],axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Language.replace(to_replace=['Portugeese','Sweedish'],value=['Portuguese','Swedish'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=600,height=300,random_state=42).generate(' '.join(df.Text))\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df.Text.apply(len)\n",
    "sns.displot(df.num_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for lang in df['Language'].unique():\n",
    "    temp = df[df['Language']== lang]\n",
    "    sizes.append(temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in sizes:\n",
    "    #if i > 4300:\n",
    "    count = count+1\n",
    "    print(i)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in df['Language'].unique():\n",
    "    temp = df[df['Language']== lang]\n",
    "    #if temp.shape[0] > 4300:\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/lang-dataset/sentences.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/kaggle/input/lang-dataset/lan_to_language.json', 'r') as f:\n",
    "    js_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lan_code'] = data['lan_code'].map(js_file).fillna(data['lan_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lan_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=[]\n",
    "for i in data['lan_code'].unique():\n",
    "    if 1000 <= df[df['Language']==i].shape[0]:\n",
    "        #print(df[df['Language']==i])\n",
    "        lang.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(columns=['Text','Language'])\n",
    "for lan in lang:\n",
    "    #l = len(df[lan])\n",
    "    #print(l)\n",
    "    #print(lang.index(lan))\n",
    "    t = data[data['lan_code']==lan][:7000]\n",
    "    #t = t.apply(name_change,axis=1)\n",
    "    t = t .drop(['id'],axis=1)\n",
    "    t = t.rename(columns={'lan_code':'Language','sentence':'Text'})\n",
    "    t = t.reset_index(drop=True)\n",
    "    df_1 = pd.concat([df_1,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, wget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "wget.download(\"https://raw.githubusercontent.com/yogawicaksana/helper_prabowo/main/helper_prabowo_ml.py\",out=\"helper_prabowo_ml.py\")\n",
    "from helper_prabowo_ml import clean_html, remove_links, remove_special_characters, removeStopWords, remove_, remove_digits, lower, email_address, non_ascii, punct, hashtags\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(data,col):\n",
    "    data[col] = data[col].apply(func=clean_html)\n",
    "    data[col] = data[col].apply(func=remove_)\n",
    "    data[col] = data[col].apply(func=removeStopWords)\n",
    "    data[col] = data[col].apply(func=remove_digits)\n",
    "    data[col] = data[col].apply(func=remove_links)\n",
    "    data[col] = data[col].apply(func=remove_special_characters)\n",
    "    data[col] = data[col].apply(func=punct)\n",
    "    data[col] = data[col].apply(func=non_ascii)\n",
    "    data[col] = data[col].apply(func=email_address)\n",
    "    data[col] = data[col].apply(func=lower)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = text_preprocess(df,'Text')\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for idx, lang in enumerate(preprocessed_df.Language.unique()):\n",
    "    labels_dict[lang] = idx\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['Label'] = preprocessed_df.Language.map(labels_dict)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(preprocessed_df,test_size=0.2,random_state=42,shuffle=True,stratify=preprocessed_df.Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "xlm = TFAutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\",from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(text=train_df.Text.tolist(),\n",
    "                   max_length=max_len,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   add_special_tokens=True,\n",
    "                   return_tensors=\"tf\",\n",
    "                   return_attention_mask=True,\n",
    "                   return_token_type_ids=False,\n",
    "                   verbose=True)\n",
    "\n",
    "X_test = tokenizer(text=test_df.Text.tolist(),\n",
    "                  max_length=max_len,\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  add_special_tokens=True,\n",
    "                  return_tensors=\"tf\",\n",
    "                  return_attention_mask=True,\n",
    "                  return_token_type_ids=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n",
    "attention_mask = Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = xlm(input_ids,attention_mask=attention_mask)[0] # 0 --> final hidden state, 1 --> pooling output\n",
    "output = Flatten()(embeddings)\n",
    "output = Dense(units=1024,activation='relu')(output)\n",
    "output = Dropout(0.3)(output)\n",
    "output = Dense(units=512,activation='relu')(output)\n",
    "output = Dropout(0.2)(output)\n",
    "output = Dense(units=512,activation='relu')(output)\n",
    "output = Dropout(0.2)(output)\n",
    "output = Dense(units=128,activation='relu')(output)\n",
    "output = Dense(units=30,activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs=[input_ids,attention_mask],outputs=output)\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,'model.png',show_shapes=True,dpi=100,rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5,epsilon=2e-8,decay=0.01,clipnorm=1.0)\n",
    "loss = CategoricalCrossentropy()\n",
    "metrics = CategoricalAccuracy('balanced_accuracy')\n",
    "model.compile(loss=loss,optimizer=optimizer,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_balanced_accuracy',patience=20,verbose=1,mode='max',restore_best_weights=True)\n",
    "mc = ModelCheckpoint(filepath='checkpoint',monitor='val_balanced_accuracy',mode='max',save_best_only=True,verbose=1)\n",
    "r = model.fit(x={'input_ids': X_train['input_ids'], 'attention_mask': X_train['attention_mask']},\n",
    "              y=to_categorical(train_df.Label),\n",
    "              epochs=3,\n",
    "              batch_size=64,\n",
    "              callbacks=[es,mc],\n",
    "              validation_data=({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']},to_categorical(test_df.Label))\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'],'r',label='train loss')\n",
    "plt.plot(r.history['val_loss'],'b',label='test loss')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Categorical Crossentropy Loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('language_detector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
